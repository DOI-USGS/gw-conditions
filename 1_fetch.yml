target_default: 1_fetch

packages:
  - dataRetrieval
  - dplyr
  - readr

sources:
  - 1_fetch/src/fetch_s3.R
  - 1_fetch/src/fetch_nwis.R
  - 1_fetch/src/do_gw_fetch.R

targets:

  1_fetch:
    depends:
      - 1_fetch/out/gw_sites.rds
      - 1_fetch/out/gw_site_info.csv
      - 1_fetch/out/gw_data.csv

##-- Historic GW sites and data --##

  # All sites (including those with years < min_years)
  1_fetch/out/historic_gw_site_info_unfiltered.rds:
    command: fetch_s3(target_name, historic_unfiltered_site_info_s3_fn)
  # All data (including those with years < min_years)
  1_fetch/out/historic_gw_data_unfiltered.csv:
    command: fetch_s3(target_name, historic_unfiltered_data_s3_fn)
  # This data and the quantiles are just for sites with > min_years
  1_fetch/out/historic_gw_data_filtered.csv:
    command: fetch_s3(target_name, historic_filtered_data_s3_fn)
  
  # Only this one is absolutely necessary to run the pipeline in its current 
  # state (2_process/out/gw_daily_quantiles.csv depends on it). The others
  # will not build unless you manually build the targets.
  1_fetch/out/historic_gw_quantiles.csv:
    command: fetch_s3(target_name, historic_quantiles_s3_fn)

##-- GW sites and data for current viz time period --##
  
  1_fetch/out/gw_sites.rds:
    command: fetch_gw_sites(target_name, viz_start_date, viz_end_date, viz_bounding_box, gw_param_cd)
  1_fetch/out/gw_site_info.csv:
    command: fetch_gw_site_info(target_name, "1_fetch/out/gw_sites.rds")
  
  gw_sites:
    command: readRDS('1_fetch/out/gw_sites.rds')
  1_fetch/out/gw_data.csv:
    command: do_gw_fetch(
      final_target = target_name,
      task_makefile = I('1_fetch_tasks.yml'),
      gw_site_nums = gw_sites,
      request_limit = 200,
      '1_fetch/src/do_gw_fetch.R',
      '1_fetch/src/fetch_nwis.R')
